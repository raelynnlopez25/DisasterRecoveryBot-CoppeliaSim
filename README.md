# DisasterRecoveryBot-CoppeliaSim
Simulated robot with sensors in disaster recovery scenario using CoppeliaSim

LICENSE AND USAGE
--------
This project was created as part of my undergraduate computer science coursework and is provided here for demonstration purposes only. No permission is granted to copy, modify, distribute, or use this code in any form, including commercial or non-commercial applications. If you'd like to reference this project or discuss it, feel free to reach out via GitHub or LinkedIn.

DESCRIPTION
--------
DISASTER RECOVERY ENVIRONMENT
The disaster recovery environment is an office space enclosed by walls in all four directions. An earthquake has taken place in this scenario, and there may be victims trapped within the office space. Currently, there is no access into this space large enough for humans to fit through and search for victims or survivors. There are a multitude of obstacles in this environment, including: exterior walls, two interior walls, a plant, a table, and a cabinet. The colored pillars shown represent the human victims in the disaster scenario. The components within the environment are solid and the robot may collide with them.

IMPROVED DISASTER RECOVERY
The robot is designed to move across a floor in any direction, detect and avoid obstacles, and identify victims within the scene. When the robot encounters an obstacle, it backs up, rotates, and heads in another direction until it encounters the next collision/obstacle. This allows the robot to move around obstacles and discover new areas. Additionally, the robot can identify a victim within a certain range and print the details of the victim’s location to the console.
The robot improves disaster recovery in this scenario because it can enter the office space (any doors or windows were blocked by debris in the earthquake) through a small opening that human medical/recovery assistance is currently unable to gain access to. Without the robot, the human recovery assistance may not be aware of where there are trapped victims and therefore could result in additional casualties if the victims were stuck and undiscovered for an extended period of time.
 
ARCHITECTURE
The disaster recovery robot was developed from the foundational Bubble Rob tutorial scene and code. The robot now includes two sensors: “sensingNose” and “sensingNose_To_Detect”. 
The former is a short-range proximity sensor that is intended to detect collisions in the environment and make the necessary adjustments to keep the robot moving (avoid getting stuck attempting to move into a solid object). When the sensor detects something, the robot backs up, turns counterclockwise, and returns to moving forward until the next collision is detected (i.e. sensingNose detects an object).     
The latter is a wider range sensor that detects objects over a larger x and y radius, further away from the robot. The purpose of this sensor is to identify victim objects and print a message to the console including a notification of a victim being found as well as their xyz coordinates. An additional adjustment made to the robot included reducing the amount that it rotates after encountering a collision. This reduced angle of rotation increases the likelihood that the robot will encounter new territory in most cases, which is valuable for disaster recovery efforts and locating victims.

REPRESENTATION OF THE ENVIRONMENT
The disaster recovery robot maintains an internal representation of the environment through its real-time sensor readings. It continuously uses two proximity sensors that collect knowledge about the environment and influence the robot’s decisions, as described in the previous section. The sensingNose sensor value updates to a value > 0 when an object is within the sensor range, thus identifying when a collision has occurred and implicating the need to change movement directions. This identification triggers a change in the robot’s movement behavior: a sequence of backward, rotating, then forward movement actions to move around or away from the obstacle with which the robot collided. The other sensor is designed to identify objects within its range and check whether the objects follow a specific naming scheme: “^Victim_To_Detect”. If the detected object does follow that naming scheme, the robot has identified a victim and can therefore report the victim’s xyz coordinates to the console. This is an important functionality because it helps distinguish between the numerous, random obstacles it may encounter and victims in the searching process, thus supporting the overall goal of the disaster recovery robot. The robot maintains its position using “sim.getObjectPosition(disasterRecoveryBotBase)”, which allows it to track where it is relative to detected victims and obstacles. Thus, the robot's internal representation is based on reactive sensing and real-time decision-making, allowing it to navigate dynamically in an unknown disaster environment. Although the robot does not store a mapped representation of the environment, its real-time sensor-based approach allows it to adapt to dynamic conditions, such as shifting obstacles after an earthquake.

REASONING, KNOWLEDGE REPRESENTATION, UNCERTAINTY, AND INTELLIGENCE
Knowledge
The robot collects information about its environment by using two different sensors: sensingNose_To_Detect and sensingNose. Each sensor serves a unique function. SensingNose_To_Detect is specifically designed to identify victims as the robot moves along its route within the scene map. The sensingNose sensor is designed to trigger a change in movement procedure when the robot encounters a collision with an object. Therefore, the robot receives both information about discovered victims (and their locations) as well as when it needs to change directions to keep exploring.

Reasoning
The robot makes decisions based on the information it receives from its two sensors. For example, if the sensingNose value is not > 0, the robot does not need to change directions and can continue moving in a straight line. Another example of reasoning includes when the sensingNose_To_Detect moves within range of an object. The robot determines whether or not to print the “Human found!....” message based on the sensor’s evaluation of the object’s naming scheme: does it fit the predefined criteria to print the message or not? In other words, the robot knows when to print the message and when not to. 
    
Uncertainty
The robot does not know the layout of the scene map prior to roaming and therefore cannot preemptively plan a route that avoids collisions with obstacles. The robot demonstrates adaptability to uncertain environments via the use of the sensingNose that a) detects collisions with objects and b) redirects the robot in another direction to keep roaming. Therefore, even when encountering “unexpected” collisions along a path, the robot can correct course and keep exploring.

Intelligence
In order to aid disaster recovery efforts, the robot must overcome the challenges of not knowing the surrounding environment: where the obstacles are, where the victims are located, or the ideal travel path(s) ahead of time. By utilizing its sensors, the robot is able to identify and handle collisions by adjusting course to explore in a new direction. Additionally, the robot is able to perceive an object and identify whether it is simply an obstacle or also a victim. These functionalities prove the robots intelligence to support the disaster recovery efforts.
